INFO: Number of nodes: 1
INFO: GPUs per node (this node): 1
INFO: Total GPUs for training (world size): 1
Attempting to find latest checkpoint iteration...
Found latest iteration: 0
START!
--- PBS_NODEFILE (/var/spool/pbs/aux/565166.opbs) content ---
mg0056
--- Unique nodes in /tmp/565166.9763/tmp.HheZjXvHQF ---
mg0056
--- nvidia-smi (on submission node) ---
Wed Jul  2 15:37:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 120GB             On  |   00000009:01:00.0 Off |                    0 |
| N/A   23C    P0             82W /  700W |      65MiB /  97871MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Master Addr: mg0056
Master Port: 29500
Number of Nodes for torchrun: 1
Processes per Node for torchrun: 1
Python script: /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/pretrain_bert.py
Logging to: /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/log//bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-100-mbs-100-gpu-1-zero-0-mp-1-pp-1-nopp_mg0056_2025.07.02-15.37.44.log
--------------------------------------------------------------------------
mpirun --hostfile /tmp/565166.9763/tmp.HheZjXvHQF -np 1 -npernode 1 --map-by node -x CUDA_HOME -x LD_LIBRARY_PATH -x PATH \
torchrun     --nnodes 1     --nproc_per_node 1     --rdzv_id 565166.opbs     --rdzv_backend c10d     --rdzv_endpoint 172.17.1.56:29500     /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/pretrain_bert.py          --bert-no-binary-head     --disable-bias-linear     --override-opt_param-scheduler     --adam-beta1 0.9     --adam-beta2 0.999     --init-method-std 0.02     --tensor-model-parallel-size 1     --lr-decay-iters 213525     --lr-warmup-iters 2000     --micro-batch-size 100     --global-batch-size 100     --num-layers 12     --hidden-size 768     --num-attention-heads 12     --seq-length 1024     --max-position-embeddings 1024     --train-iters 213525     --lr 1e-4     --min-lr 1e-5     --lr-decay-style linear     --split 949,50,1     --log-interval 10000     --eval-interval 1000     --eval-iters 100     --save-interval 213     --weight-decay 1e-2     --clip-grad 1.0     --num-workers 4     --fp16     --geglu     --layernorm-embedding     --load /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/checkpoint/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-100-mbs-100-gpu-1-zero-0-mp-1-pp-1-nopp     --save /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/checkpoint/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-100-mbs-100-gpu-1-zero-0-mp-1-pp-1-nopp     --tensorboard-queue-size 1     --log-timers-to-tensorboard     --log-batch-size-to-tensorboard     --log-validation-ppl-to-tensorboard     --tensorboard-dir /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/tensorboard/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-100-mbs-100-gpu-1-zero-0-mp-1-pp-1-nopp_mg0056_2025.07.02-15.37.44     --use-switch-attention     --global-attn-every-n-layers 3     --local-window-size 128     --wandb-project deepspeed-megatron     --use-flash-attn-v2     --use-switch-attention-rope     --no-position-embedding     --wandb-exp-name full-pubmed-1-epoch     --wandb-save-dir /work/gg17/a97006/250519_modern_bert_0/users/a97006/project/bert_with_pile     --log-optimizer-states-to-tensorboard          --vocab-file /work/gg17/a97006/250519_modern_bert_0/tokenizer/vocab_30000.txt     --data-path /work/gg17/a97006/250519_modern_bert_0/preprocessed/pubmed/pubmed_30000-1024/pubmed_text_document     --num-workers 128     --data-impl mmap          --deepspeed     --deepspeed_config /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/examples_deepspeed/bert_with_pile/ds_config_bert_bsz100_mbsz100_log10000_zero0.json     --zero-stage 0     --pipeline-model-parallel-size 1     --no-pipeline-parallel
--------------------------------------------------------------------------
/work/opt/local/aarch64/cores/cuda/12.6
--------------------------------------------------------------------------
HOST: mg0056
HOST_IP: 172.17.1.56
--------------------------------------------------------------------------
mpirun LD_LIBRARY_PATH;PATH \
END OF SCRIPT
INFO: Number of nodes: 1
INFO: GPUs per node (this node): 1
INFO: Total GPUs for training (world size): 1
Attempting to find latest checkpoint iteration...
Found latest iteration: 0
START!
--- PBS_NODEFILE (/var/spool/pbs/aux/565224.opbs) content ---
mg0056
--- Unique nodes in /tmp/565224.5618/tmp.mUo4fUgcId ---
mg0056
--- nvidia-smi (on submission node) ---
Wed Jul  2 15:44:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 120GB             On  |   00000009:01:00.0 Off |                    0 |
| N/A   23C    P0             82W /  700W |      54MiB /  97871MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Master Addr: mg0056
Master Port: 29500
Number of Nodes for torchrun: 1
Processes per Node for torchrun: 1
Python script: /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/pretrain_bert.py
Logging to: /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/log//bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-64-mbs-64-gpu-1-zero-0-mp-1-pp-1-nopp_mg0056_2025.07.02-15.44.00.log
--------------------------------------------------------------------------
mpirun --hostfile /tmp/565224.5618/tmp.mUo4fUgcId -np 1 -npernode 1 --map-by node -x CUDA_HOME -x LD_LIBRARY_PATH -x PATH \
torchrun     --nnodes 1     --nproc_per_node 1     --rdzv_id 565224.opbs     --rdzv_backend c10d     --rdzv_endpoint 172.17.1.56:29500     /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/pretrain_bert.py          --bert-no-binary-head     --disable-bias-linear     --override-opt_param-scheduler     --adam-beta1 0.9     --adam-beta2 0.999     --init-method-std 0.02     --tensor-model-parallel-size 1     --lr-decay-iters 213525     --lr-warmup-iters 2000     --micro-batch-size 64     --global-batch-size 64     --num-layers 12     --hidden-size 768     --num-attention-heads 12     --seq-length 1024     --max-position-embeddings 1024     --train-iters 213525     --lr 1e-4     --min-lr 1e-5     --lr-decay-style linear     --split 949,50,1     --log-interval 10000     --eval-interval 1000     --eval-iters 100     --save-interval 213     --weight-decay 1e-2     --clip-grad 1.0     --num-workers 4     --fp16     --geglu     --layernorm-embedding     --load /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/checkpoint/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-64-mbs-64-gpu-1-zero-0-mp-1-pp-1-nopp     --save /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/checkpoint/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-64-mbs-64-gpu-1-zero-0-mp-1-pp-1-nopp     --tensorboard-queue-size 1     --log-timers-to-tensorboard     --log-batch-size-to-tensorboard     --log-validation-ppl-to-tensorboard     --tensorboard-dir /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/users/a97006/project/bert_with_pile/tensorboard/bert-full-pubmed-30000-0.11B-iters-2M-lr-1e-4-min-1e-5-wmup-2000-dcy-2M-sty-linear-gbs-64-mbs-64-gpu-1-zero-0-mp-1-pp-1-nopp_mg0056_2025.07.02-15.44.00     --use-switch-attention     --global-attn-every-n-layers 3     --local-window-size 128     --wandb-project deepspeed-megatron     --use-flash-attn-v2     --use-switch-attention-rope     --no-position-embedding     --wandb-exp-name full-pubmed-1-epoch     --wandb-save-dir /work/gg17/a97006/250519_modern_bert_0/users/a97006/project/bert_with_pile     --log-optimizer-states-to-tensorboard          --vocab-file /work/gg17/a97006/250519_modern_bert_0/tokenizer/vocab_30000.txt     --data-path /work/gg17/a97006/250519_modern_bert_0/preprocessed/pubmed/pubmed_30000-1024/pubmed_text_document     --num-workers 128     --data-impl mmap          --deepspeed     --deepspeed_config /work/gg17/a97006/250519_modern_bert_0/Inhouse-Megatron-DeepSpeed/examples_deepspeed/bert_with_pile/ds_config_bert_bsz64_mbsz64_log10000_zero0.json     --zero-stage 0     --pipeline-model-parallel-size 1     --no-pipeline-parallel
--------------------------------------------------------------------------
/work/opt/local/aarch64/cores/cuda/12.6
--------------------------------------------------------------------------
HOST: mg0056
HOST_IP: 172.17.1.56
--------------------------------------------------------------------------
mpirun LD_LIBRARY_PATH;PATH \
